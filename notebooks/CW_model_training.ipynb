{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CW_model_training.ipynb \n",
    "\n",
    "This is based on the original notebook by the main author of the paper (`RADIPOP_model_training.ipynb`).\n",
    "Since I need to reuse it on new data, I might as well clean it up a bit. \n",
    "\n",
    "\n",
    "However, currently it is not finished... #TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numba\n",
    "from typing import Literal \n",
    "from glob import glob\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, r2_score, RocCurveDisplay\n",
    "# see https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.stats import ttest_ind\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import skopt\n",
    "import time\n",
    "import re \n",
    "\n",
    "import radipop_utils \n",
    "import radipop_utils.visualization\n",
    "import radipop_utils.features\n",
    "from radipop_utils.features import SpearmanReducerCont\n",
    "import radipop_utils.utils\n",
    "from radipop_utils.utils import get_files_dict_by_regex_pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load user/ system specific env variables:\n",
    "from dotenv import dotenv_values, find_dotenv\n",
    "config = dotenv_values(find_dotenv())  # load environment variables as dictionary\n",
    "\n",
    "path = Path(os.path.abspath(radipop_utils.__file__))\n",
    "RADIPOP_PACKAGE_ROOT = path.parent.parent\n",
    "\n",
    "\n",
    "##------  You will likely need to change this \n",
    "DATA_ROOT_DIRECTORY = Path(config[\"DATA_ROOT_DIRECTORY\"])\n",
    "OUTDIR = DATA_ROOT_DIRECTORY / \"radiomics\" / \"Dataset125_LSS\" \n",
    "##-----------\n",
    "\n",
    "os.makedirs(OUTDIR / \"model_training\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the the data: \n",
    "- load radiomics and HVPG values \n",
    "- utilize our custom split (previously defined and stratified on sex, scanner, status)\n",
    "- normalized the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features and combine with predicted values: \n",
    "\n",
    "def get_HVPG_values_and_radiomics_paths():\n",
    "\n",
    "    # TODO change to strict and rerun\n",
    "    df = pd.read_excel(RADIPOP_PACKAGE_ROOT / \"data\" / \"file_paths_and_hvpg_data.xlsx\")\n",
    "\n",
    "    DATA_ROOT_DIRECTORY = Path(config[\"DATA_ROOT_DIRECTORY\"])\n",
    "    base_path = DATA_ROOT_DIRECTORY / \"radiomics\" / \"Dataset125_LSS\" / \"radipop\"\n",
    "    dct_paths = get_files_dict_by_regex_pattern(base_path, regex_pattern=\"^Features_liver\", strict=False)\n",
    "    df_dirs_features_liver = pd.DataFrame.from_records({ 'id': dct_paths.keys(), 'radiomics-features: liver': dct_paths.values() })\n",
    "\n",
    "    dct_paths = get_files_dict_by_regex_pattern(base_path, regex_pattern=\"^Features_spleen\", strict=False)\n",
    "    df_dirs_features_spleen = pd.DataFrame.from_records({ 'id': dct_paths.keys(), 'radiomics-features: spleen': dct_paths.values() })\n",
    "\n",
    "    # Merge the DataFrames on the 'id' column\n",
    "    df = df.merge(df_dirs_features_liver, on='id', how='inner').merge(df_dirs_features_spleen, on='id', how='inner')\n",
    "    \n",
    "    # drop unnamed columns (index)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # TODO rm after strict \n",
    "    df['radiomics-features: liver'] = df['radiomics-features: liver'].apply(lambda x: x[0] if len(x)==1 else pd.NA)\n",
    "    df['radiomics-features: spleen'] = df['radiomics-features: spleen'].apply(lambda x: x[0] if len(x)==1 else pd.NA)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def read_and_combined_radiomics_features(df_paths: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    df_paths = df_paths.reset_index(drop=True)\n",
    "    for i in range(len(df_paths)):\n",
    "\n",
    "        patientid = df_paths.loc[i, 'id']\n",
    "        file_r1 = df_paths.loc[i, 'radiomics-features: liver']\n",
    "        file_r2 = df_paths.loc[i, 'radiomics-features: spleen']\n",
    "\n",
    "        df_r1 = pd.read_excel(file_r1)  # these all have just a single row of data\n",
    "        df_r2 = pd.read_excel(file_r2)  \n",
    "        assert len(df_r1) == 1\n",
    "        assert len(df_r2) == 1\n",
    "\n",
    "        df_r1 = df_r1.loc[:, ~df_r1.columns.str.contains('^Unnamed')]\n",
    "        df_r2 = df_r2.loc[:, ~df_r2.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        # Add prefixes to the columns\n",
    "        df_r1 = df_r1.add_prefix('liver: ')\n",
    "        df_r2 = df_r2.add_prefix('spleen: ')\n",
    "\n",
    "        combined_df = pd.concat([df_r1, df_r2], axis=1)\n",
    "        combined_df['id'] = patientid\n",
    "        \n",
    "        dfs.append(combined_df)\n",
    "        \n",
    "    df_radiomics = pd.concat(dfs, axis=0)\n",
    "\n",
    "    # Move column \"patient_id\" to be the first column\n",
    "    cols = list(df_radiomics.columns)\n",
    "    cols.insert(0, cols.pop(cols.index('id')))\n",
    "    df_radiomics = df_radiomics[cols].reset_index(drop=True)\n",
    "\n",
    "    return df_radiomics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_HVPG_values_and_radiomics_paths()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if the data is complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop not completed radiomics for now \n",
    "df_  = df.dropna(subset=[\"radiomics-features: liver\", \"radiomics-features: spleen\"])\n",
    "\n",
    "# load radiomics data for completed calcs\n",
    "df_radiomics = read_and_combined_radiomics_features(df_)\n",
    "df_merged = df.merge(df_radiomics, on='id', how='inner')\n",
    "\n",
    "# final filtered dataframe \n",
    "dff = df_merged.filter(regex=\"^id|^y|^set type|^Tr split|^liver|^spleen\")\n",
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data was already done \n",
    "m_Tr = dff[\"set type\"] == \"Tr\"\n",
    "m_iTs = dff[\"set type\"] == \"internal Ts\"\n",
    "m_eTs = dff[\"set type\"] == \"Ts\"\n",
    "\n",
    "df_Tr  = dff[m_Tr]\n",
    "df_iTs = dff[m_iTs]\n",
    "df_eTs = dff[m_eTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_Tr)\n",
    "display(df_iTs)\n",
    "display(df_eTs)\n",
    "\n",
    "set(df[\"set type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract indices for stratified CV:\n",
    "\n",
    "df_Tr = df_Tr.reset_index(drop=True)\n",
    "split_indices_CV5_Tr = []\n",
    "for i in range(5):\n",
    "    m = df_Tr[\"Tr split\"] == i\n",
    "    idx_split_tr = df_Tr[m].index.to_numpy()\n",
    "    idx_split_ts = df_Tr[~m].index.to_numpy()\n",
    "    split_indices_CV5_Tr.append([idx_split_tr, idx_split_ts])\n",
    "    \n",
    "\n",
    "# idx_split_tr = split_indices_CV5_Tr[1][0]\n",
    "# idx_split_ts = split_indices_CV5_Tr[1][1]\n",
    "# df_Tr.iloc[idx_split_tr, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract np arrays\n",
    "X_Tr,  Y_Tr  = df_Tr.filter(regex=\"^liver|^spleen\").values, df_Tr[\"y\"].values\n",
    "X_iTs, Y_iTs = df_iTs.filter(regex=\"^liver|^spleen\").values, df_iTs[\"y\"].values\n",
    "#X_eTs, Y_eTs = df_eTs.filter(regex=\"^liver|^spleen\").values, df_eTs[\"y\"].values\n",
    "\n",
    "\n",
    "# Normalize mostly for numerical stability\n",
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(X_Tr)  # fit on trainig data only\n",
    "\n",
    "X_Tr = transformer.transform(X_Tr)\n",
    "X_iTs = transformer.transform(X_iTs)\n",
    "#X_eTs = transformer.transform(X_eTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot dendrogram\n",
    "corr = spearmanr(X_Tr).correlation\n",
    "\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# plt.matshow(corr)\n",
    "# plt.show()\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "plt.figure()\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, no_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide on a rought range for the cut parameters for dendrogram\n",
    "split_params = [0.5, 0.75, 1, 2.75,  5, 7.5, 10]\n",
    "for split_param in split_params:\n",
    "    selector = SpearmanReducerCont(split_param=split_param)\n",
    "    print(f\"Selected features at height {split_param}:\", len(selector.fit(X_Tr, Y_Tr).selected_features))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit on `Tr` data with CV and estimate best model + hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for hyperparameters\n",
    "param_bounds_rf = {\n",
    "    'feature_selection__split_param' : skopt.space.Real(1,5, prior = \"uniform\"),\n",
    "    'regression' : [RandomForestRegressor(random_state=2023)],\n",
    "    'regression__n_estimators': skopt.space.Integer(100, 2000),\n",
    "    'regression__max_depth': skopt.space.Integer(1, 50),\n",
    "    'regression__min_samples_split': skopt.space.Integer(2, 25)#,\n",
    "}\n",
    "\n",
    "\n",
    "param_bounds_en = {\n",
    "                 'feature_selection__split_param' : skopt.space.Real(1,5, prior = \"uniform\"),\n",
    "                 'regression' : [ElasticNet(random_state=2023)],\n",
    "                 'regression__alpha': skopt.space.Real(0.0001, 1.0, 'uniform'),\n",
    "                 'regression__l1_ratio': skopt.space.Real(0, 1.0, 'uniform')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline\n",
    "reg = Pipeline([\n",
    "  #('scaler', StandardScaler()),  \n",
    "  ('feature_selection', SpearmanReducerCont()),\n",
    "  ('regression', RandomForestRegressor())\n",
    "]) \n",
    "\n",
    "# cv5 = KFold(5, shuffle=True, random_state=2023)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#try out models\n",
    "opt0 = skopt.BayesSearchCV(\n",
    "    reg,\n",
    "    [(param_bounds_en, 10), (param_bounds_rf, 10)],\n",
    "    cv=split_indices_CV5_Tr,\n",
    "    scoring=\"r2\",  # \"neg_root_mean_squared_error\"\n",
    "    verbose=True,\n",
    "    random_state=2023,\n",
    "    n_jobs = 6\n",
    ")\n",
    "opt0.fit(X_Tr, Y_Tr)\n",
    "\n",
    "display(opt0.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(opt0.cv_results_)\n",
    "cv_res\n",
    "cv_res.iloc[:, :].reset_index().loc[:, \"mean_test_score\"].plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_res.to_excel(OUTDIR / \"model_training/ \"Bayesian_results_10_iterations_RFvsEN.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline\n",
    "reg = Pipeline([\n",
    "  #('scaler', StandardScaler()),  \n",
    "  ('feature_selection', SpearmanReducerCont()),\n",
    "  ('regression', RandomForestRegressor())\n",
    "]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set params\n",
    "np.random.seed(2023)\n",
    "print(opt0.best_params_)\n",
    "reg.set_params(**opt0.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalution metics on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set best performing en model (rf model has already been set)\n",
    "#create a pipeline\n",
    "reg_EN = Pipeline([\n",
    "  #('scaler', StandardScaler()),  \n",
    "  ('feature_selection', SpearmanReducerCont()),\n",
    "  ('regression', ElasticNet())\n",
    "]) \n",
    "\n",
    "reg_EN.set_params(**cv_res.iloc[5, :].params)\n",
    "\n",
    "#run 5 fold cv\n",
    "rf_train_res = np.array([])\n",
    "en_train_res = np.array([])\n",
    "obs = np.array([])\n",
    "\n",
    "for train, test in split_indices_CV5_Tr:\n",
    "    \n",
    "    #rf\n",
    "    reg.fit(X_Tr[train], Y_Tr[train])\n",
    "    rf_train_res = np.append(rf_train_res, reg.predict(X_Tr[test]))\n",
    "    \n",
    "    #en\n",
    "    reg_EN.fit(X_Tr[train], Y_Tr[train])\n",
    "    en_train_res = np.append(en_train_res, reg_EN.predict(X_Tr[test]))\n",
    "    \n",
    "    #obs\n",
    "    obs = np.append(obs, Y_Tr[test])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_training = pd.DataFrame({\"True_HVPG\" : obs, \n",
    "                             \"RF_HVPG\" : rf_train_res,\n",
    "                             \"EN_HVPG\" : en_train_res})\n",
    "\n",
    "display(res_training)\n",
    "\n",
    "# res_training.to_excel(OUTDIR / \"model_training/CV_results_training_cohort.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO finish for \n",
    "r2_score(res_training[\"True_HVPG\"], res_training[\"RF_HVPG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_eTs, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvpg_cat = [0 if x < 10 else 1 for x in meta_test.HVPG]\n",
    "roc_auc_score(hvpg_cat, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.named_steps[\"regression\"].feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(reg.named_steps[\"regression\"].feature_importances_>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export feature importances\n",
    "selector = SpearmanReducerCont(opt0.best_params_['feature_selection__split_param'])\n",
    "selector.fit(X_Tr, Y_Tr)\n",
    "features = np.array([x for x in data_train.columns if not x == \"ID\"])\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": features[selector.selected_features],\n",
    "    \"importance\": reg.named_steps[\"regression\"].feature_importances_})\n",
    "feat_imp.sort_values(\"importance\", ascending=False).to_excel(OUTDIR / \"model_training\" / \"Feature_importances_RF_regressor.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export prediction data\n",
    "meta_test.loc[:,\"rHVPG\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test.to_excel(OUTDIR / \"model_training\" / \"Metadata_with_predictions.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate external validation set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #external validation\n",
    "# data_val = pd.read_excel(\"External_validation_features.xlsx\")\n",
    "\n",
    "# #read hvpg for external validation cohort\n",
    "# data_ext_val_hvpg = pd.read_excel(\"D:/FINAL.External_validation_cohort_RADIPOP_with_additions.xlsx\")\n",
    "\n",
    "# data_ext_val_hvpg = pd.DataFrame({\"ID\" : [\"V \"+str(x) for x in data_ext_val_hvpg[\"ID paris\"]],\n",
    "#                                   \"HVPG\" : data_ext_val_hvpg[\"HVPG\"]})\n",
    "\n",
    "# data_ext_val_hvpg = pd.merge(pd.DataFrame({\"ID\" : data_val[\"ID\"]}), data_ext_val_hvpg, on=\"ID\")\n",
    "\n",
    "# #synchronize columns\n",
    "# data_val = data_val.loc[:, data_all.columns]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = data_val.loc[:, [x for x in data_test.columns if not x == \"ID\"]].values, data_ext_val_hvpg.HVPG.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_val = reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(res_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_val, res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvpg_cat = [0 if x < 10 else 1 for x in Y_val]\n",
    "roc_auc_score(hvpg_cat, res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = np.array(Y_val, dtype = float), y = res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_val_hvpg[\"rHVPG\"] = res_val\n",
    "data_ext_val_hvpg.to_excel(\"Metadata_ext_with_predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(\"Final_model\")\n",
    "with open(os.path.join(\"Final_model\", \"SpearmanRed1_RF_10.p\"), \"wb\") as fp:\n",
    "    pickle.dump(reg, fp)\n",
    "    \n",
    "with open(os.path.join(\"Final_model\", \"SpearmanRed1_RF_10_opt.p\"), \"wb\") as fp:\n",
    "    pickle.dump(opt0, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt3-9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
